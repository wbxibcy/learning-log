# 2023-5-20

## 神经网络

### 激活函数

全连接层的变换是线性变换。

激活函数:
$$
\sigma(x)=\frac{1}{1+exp(-x)}
$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### 损失函数

损失指示学习阶段中某个时间点的神经网络的性能。

基于监督数据（学习阶段获得的正确解数据）和神经网络的预测结果将模型的恶劣程度作为标量（单一数据）计算出来，得到的就是损失。

计算神经网络的损失要使用==损失函数==，进行多类别分类的神经网络通常使用==交叉熵误差==作为损失函数。

Softmax函数：
$$
y_k = \frac{exp(s_k)}{\sum^{n}_{i=1}exp(s_i)}
$$
交叉熵误差:
$$
L=-\sum_{k}t_klogy_k
$$

### 权重更新

随机梯度下降(==SGD==)
$$
\bf{w}\leftarrow \bf{w}-\eta\frac{\partial \bf{l}}{\partial \bf{w}}
$$

```python
class SGD:
    '''
    随机梯度下降法（Stochastic Gradient Descent）
    '''
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for i in range(len(params)):
            params[i] -= self.lr * grads[i]
```

## 自然语言和单词的分布式表示

### 同义词词典

具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中。

### 基于计数的方法

使用语料库（corpus）

#### 分布式假设

某个单词的含义由它周围的单词形成

### 点互信息

$$
PMI(x,y)=log_2\frac{P(x,y)}{p(x)P(y)}
$$

值越高，表明相关性越强
$$
PPMI(x,y)=max(0, PMI(x,y))
$$

### 向量降维

在尽量保留“重要信息”的基础上减少向量维度。

#### 奇异值分解（SVD）

$$
\bf{x}=\bf{USV^T}
$$

**U**就是降维后的向量。

## word2vec

基于推理的方法

- 基于计数的方法的问题

具体来说，先生成所有单词的共现矩阵，再对这个矩阵进行SVD，以获得密集向量（单词的分布式表示），但是，基于计数的方法再处理大规模语料库时会出现问题。

- 基于计数的方法一次性处理全部学习数据；基于推理的方法使用部分学习数据逐步学习。

### CBOW模型的实现

```python
class SimpleCBOW:
    def __init__(self, vocab_size, hidden_size):
        V, H = vocab_size, hidden_size

        # 初始化权重
        W_in = 0.01 * np.random.randn(V, H).astype('f')
        W_out = 0.01 * np.random.randn(H, V).astype('f')

        # 生成层
        self.in_layer0 = MatMul(W_in)
        self.in_layer1 = MatMul(W_in)
        self.out_layer = MatMul(W_out)
        self.loss_layer = SoftmaxWithLoss()

        # 将所有的权重和梯度整理到列表中
        layers = [self.in_layer0, self.in_layer1, self.out_layer]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # 将单词的分布式表示设置为成员变量
        self.word_vecs = W_in

    def forward(self, contexts, target):
        h0 = self.in_layer0.forward(contexts[:, 0])
        h1 = self.in_layer1.forward(contexts[:, 1])
        h = (h0 + h1) * 0.5
        score = self.out_layer.forward(h)
        loss = self.loss_layer.forward(score, target)
        return loss

    def backward(self, dout=1):
        ds = self.loss_layer.backward(dout)
        da = self.out_layer.backward(ds)
        da *= 0.5
        self.in_layer1.backward(da)
        self.in_layer0.backward(da)
        return None

```

(需要导入包)

### skip-gram模型

$w_t$预测上下文$w_{t-1}$和$w_{t+1}$的情况
$$
P(w_{t-1}, w_{t+1}|w_t)
$$
假设上下文单词之间没有相关性
$$
P(w_{t-1}, w_{t+1}|w_t)=P(w_{t-1}|w_t)P(w_{t+1}|w_t)
$$


代入交叉熵损失函数
$$
L=-(logP(w_{t-1}|w_t)+logP(w_{t+1}|w_t))
$$


### 总结

从单词的分布式表示的准确度来看，在大多数情况下，skip_gram模型的结果更好。

## word2vec的高速化

### Embedding层

一个从权重参数中抽取"单词ID对应行（向量）"的层

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out

    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0
        if GPU:
            np.scatter_add(dW, self.idx, dout)
        else:
            np.add.at(dW, self.idx, dout)
        return None
```

### 负采样（negative sampling）

无论词汇量有多大，都可以使得计算量保持较低或者恒定。

这个方法的关键思想在于二分类，准确来讲，是用二分类来拟合多分类

#### 负采样的实现

```python
class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, h, target):
        batch_size = target.shape[0]
        negative_sample = self.sampler.get_negative_sample(target)

        # 正例的正向传播
        score = self.embed_dot_layers[0].forward(h, target)
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label)

        # 负例的正向传播
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, i]
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)

        return loss

    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)

        return dh

```

## RNN

### 语言模型

语言模型给出了单词序列发生的概率，具体来说，就是使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的单词序列。

RNN独有的机制：

无论上下文有多长，都能将上下文信息记住，因此，使用RNN可以处理任意长度的时序数据。

RNN正向传播的数学式：
$$
h_t=tanh(h_{t-1}W_h+x_tW_x+b)
$$

```python
class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None

    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b
        h_next = np.tanh(t)

        self.cache = (x, h_prev, h_next)
        return h_next
```

反向传播：

```python
def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)
        db = np.sum(dt, axis=0)
        dWh = np.dot(h_prev.T, dt)
        dh_prev = np.dot(dt, Wh.T)
        dWx = np.dot(x.T, dt)
        dx = np.dot(dt, Wx.T)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

整个RNN的实现代码:

```python
class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None

    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b
        h_next = np.tanh(t)

        self.cache = (x, h_prev, h_next)
        return h_next

    def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)
        db = np.sum(dt, axis=0)
        dWh = np.dot(h_prev.T, dt)
        dh_prev = np.dot(dt, Wh.T)
        dWx = np.dot(x.T, dt)
        dx = np.dot(dt, Wx.T)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

### 语言模型评价

#### 困惑度

困惑度常被用在评价语言模型的预测性能的指标。

困惑度越少越好。

#### 分叉度

分叉度是指下一个可以选择的选项的数量（下一个可能出现的单词的候选个数）。

==所以, 困惑度越小，分叉度越小，表明模型越好。==

## Gated RNN

RNN的梯度爆炸和梯度消失的问题

解决梯度爆炸有既定的方法，称为梯度裁剪。

### Gate的功能

Gate是"门"的意思，也就是说，它就像将门打开或合上一样，控制数据的流动，但是重点在于，这个门可以有开合程度，而且这个开合程度是可以（自动）从数据中学习到的。

在大多数情况下，门使用sigmoid函数作为激活函数，而包含实质信息的数据则使用tanh函数作为激活函数。

### LSTM的实现

```python
class LSTM:
    def __init__(self, Wx, Wh, b):
        '''

        Parameters
        ----------
        Wx: 输入`x`用的权重参数（整合了4个权重）
        Wh: 隐藏状态`h`用的权重参数（整合了4个权重）
        b: 偏置（整合了4个偏置）
        '''
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None

    def forward(self, x, h_prev, c_prev):
        Wx, Wh, b = self.params
        N, H = h_prev.shape

        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b

        f = A[:, :H]
        g = A[:, H:2*H]
        i = A[:, 2*H:3*H]
        o = A[:, 3*H:]

        f = sigmoid(f)
        g = np.tanh(g)
        i = sigmoid(i)
        o = sigmoid(o)

        c_next = f * c_prev + g * i
        h_next = o * np.tanh(c_next)

        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)
        return h_next, c_next

    def backward(self, dh_next, dc_next):
        Wx, Wh, b = self.params
        x, h_prev, c_prev, i, f, g, o, c_next = self.cache

        tanh_c_next = np.tanh(c_next)

        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)

        dc_prev = ds * f

        di = ds * g
        df = ds * c_prev
        do = dh_next * tanh_c_next
        dg = ds * i

        di *= i * (1 - i)
        df *= f * (1 - f)
        do *= o * (1 - o)
        dg *= (1 - g ** 2)

        dA = np.hstack((df, dg, di, do))

        dWh = np.dot(h_prev.T, dA)
        dWx = np.dot(x.T, dA)
        db = dA.sum(axis=0)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        dx = np.dot(dA, Wx.T)
        dh_prev = np.dot(dA, Wh.T)

        return dx, dh_prev, dc_prev
```

### 改进

LSTM的多层化、Dropout和权重共享等技巧可以有效改进语言模型。

## 基于RNN生成文本

```python
# 继承Rnnlm类
class RnnlmGen(Rnnlm):
    def generate(self, start_id, skip_ids=None, sample_size=100):
        word_ids = [start_id]

        x = start_id
        while len(word_ids) < sample_size:
            x = np.array(x).reshape(1, 1)
            score = self.predict(x)
            p = softmax(score.flatten())

            sampled = np.random.choice(len(p), size=1, p=p)
            if (skip_ids is None) or (sampled not in skip_ids):
                x = sampled
                word_ids.append(int(x))

        return word_ids

    def get_state(self):
        return self.lstm_layer.h, self.lstm_layer.c

    def set_state(self, state):
        self.lstm_layer.set_state(*state)
```

使用`np.random.choice`表示根据概率分布来采样下一个单词。这就表示生成的文本是有概率性的，并不是确定性的。

==确定性==是指算法的结果是唯一确定的，是可以预测的。

==概率性==算法则是概率性的确定结果，因此每次结果都会有所变化。

### seq2seq模型

这个世界充满了时序数据。文本数据、音频数据和视频数据都是时序数据，但是，还有许多任务需要将一种时序数据转换成另外一种时序数据，例如：语言识别，机器翻译等。

#### 原理

seq2seq模型也可以说是Encoder-Decoder模型。

这模型有两个模块，一个编码器（Encoder）和一个解码器（Decoder）。

编码器对数据进行编码，解码器对编码过的数据进行解码。

##### 编码和解码

编码是基于某些既定的规则的信息转换的过程。

解码则是将被编码的信息还原到它的原始形态。

#### seq2seq的实现

##### Encoder类

```python
class Encoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)

        self.params = self.embed.params + self.lstm.params
        self.grads = self.embed.grads + self.lstm.grads
        self.hs = None

    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]

    def backward(self, dh):
        dhs = np.zeros_like(self.hs)
        dhs[:, -1, :] = dh

        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout
```

##### Decoder类

```python
class Decoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.affine = TimeAffine(affine_W, affine_b)

        self.params, self.grads = [], []
        for layer in (self.embed, self.lstm, self.affine):
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, h):
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        out = self.lstm.forward(out)
        score = self.affine.forward(out)
        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        dout = self.lstm.backward(dout)
        dout = self.embed.backward(dout)
        dh = self.lstm.dh
        return dh

    def generate(self, h, start_id, sample_size):
        sampled = []
        sample_id = start_id
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array(sample_id).reshape((1, 1))
            out = self.embed.forward(x)
            out = self.lstm.forward(out)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(int(sample_id))

        return sampled
```

#### seq2seq的改进

- 反转输入数据

```python
is_reverse = False  # True
if is_reverse:
    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]
```

- 偷窥

将集中了重要信息的编码器的输出$h$分配给解码器的其他层

## Attention

### seq2seq存在的问题

seq2seq中使用编码器对时序数据进行编码，然后将编码信息传递给解码器，所以，编码器的输出是固定长度的向量。

将编码器的全部时间的隐藏状态取出来，就可以解决固定长度的问题。

### Attention

计算表示各个单词重要度(贡献值)的权重

计算这个表示各个单词重要度的权重和单词向量$hs$的加权和，就可以获得目标向量。

结果为上下文向量，并且用符号$c$表示。

Weight Sum层

```python
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None

    def forward(self, hs, a):
        N, T, H = hs.shape

        ar = a.reshape(N, T, 1).repeat(H, axis=2)
        t = hs * ar
        c = np.sum(t, axis=1)

        self.cache = (hs, ar)
        return c

    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        da = np.sum(dar, axis=2)

        return dhs, da
```

Attention Weight层

```python
class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H).repeat(T, axis=1)
        t = hs * hr
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```

这里的计算是：

Attention Weight层关注编码器输出的各个单词向量$hs$，并计算各个单词的权重$a$，然后，Weight Sum层计算$a$和$hs$的加权和，并输出上下文向量$c$。

Attention层

```python
class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        self.attention_weight = None

    def forward(self, hs, h):
        a = self.attention_weight_layer.forward(hs, h)
        out = self.weight_sum_layer.forward(hs, a)
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```

