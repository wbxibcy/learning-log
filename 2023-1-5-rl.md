# 2023-1-5

## 机器学习的两种类型

- 预测
  - 根据数据预测所需输出（有监督学习）
  - 生成数据实例（无监督学习）
- 决策
  - 在动态环境中采取行动（强化学习）
    - 转变到新的状态
    - 获得即时奖励
    - 随着时间的推移最大化累计奖励

## 强化学习的定义

- 通过从交互中学习来实现目标的计算方法
- 三个方面
  - 感知：在某种程度上感知环境的状态
  - 行动：可以采取行动来影响状态或者达到目标
  - 目标：随着时间推移最大化累积奖励

## 系统要素

==策略==是学习智能体在特定时间的行为方式

- 是从状态到行动的映射
- 确定性策略
- 随机策略

==奖励==

- 一个定义强化学习目标的标量
- 能立即感知到什么是“好”的

==价值函数==

- 状态价值是一个标量，用于定义对于长期来说什么是“好”的
- 价值函数是对未来累积奖励的预测
  - 用于评估在给定的策略下，状态的好坏

## 强化学习智能体分类

- 基于模型的强化学习
  - 策略（和/或）价值函数
  - 环境模型

- 模型无关的强化学习
  - 策略（和/或）模型
  - 没有环境模型

## 强化学习智能体分类

- 基于价值
  - 没有价值（隐含）
  - 价值函数
- 基于策略
  - 策略
  - 没有价值函数
- Actor-Critic
  - 策略
  - 价值函数

## 探索与利用

### 序列决策任务中的一个基本问题

- 基于目前策略获取已知最优收益还是尝试不同的决策
  - `Exploitation`执行能够获得已知最优收益的决策
  - `Exploitation`尝试更多可能的决策，不一定会是最优收益

### 策略探索的一些原则

- 朴素方法
  - 添加策略噪声
- 积极初始化
- 基于不确定性的度量
  - 尝试具有不确定收益的策略，可能带来更高的收益
- 概率匹配
  - 基于概率选择最佳策略
- 状态搜索
  - 探索后续状态可能带来更高的收益

## 多臂老虎机

`MAB`问题

简化版的强化学习，多臂老虎机不存在状态信息，只有动作和奖励

### 懊悔

`懊悔`被定义为拉动当前拉杆的动作a与最优拉杆的期望奖励差

`累积懊悔`就是拉动T次拉杆后累积的懊悔总量

`MAB`问题的目标为==最大化累积奖励，等价于最小化累积懊悔==

### 衰减贪心策略

- e随着时间衰减

- 理论上对数渐进衰减

缺点

- 很难找到合适的衰减

### 积极初始化

- 有一个较高的初始值

- 有偏估计，但是随着采样增加，这个偏差带来的影响会越来越小

- 但是仍然可能陷入局部最优

### 基于不确定性测度

- 一个不确定性越大的拉杆。越具有探索的价值，有可能会是最好的策略
- 也称为`UCB`：上置信算法

### 汤普森采样算法

- 想法：根据每个动作成为最优的概率来选择动作
- 实现：根据当前每个动作的价值概率分布来采样到其价值，选择价值最大的动作

## 马尔科夫决策

### 马尔可夫决策过程

- 马尔可夫决策过程
  - 提供了一套为在==结果部分随机，部分在决策者的控制下的决策过程建模==的数学框架

- MDP形式化地描述了一种强化学习的环境
  - 环境完全可预测
  - 当前状态可以完全表征过程（马尔可夫性质）

### 马尔可夫性质

性质：

- 状态从历史中捕获了所有相关的信息
- 当状态已知的时候，可以抛开历史不谈
- 也就是说，==当前状态==是未来的充分统计量

### MDP五元组

- **MDP**可以由一个五元组表示（S,A,{P<sub>sa</sub>},Y,R ）
  - S是状态的集合
    - 比如，迷宫中的位置，游戏中的当前屏幕显示
  - A是动作的集合
    - 比如，向WASD移动，手柄操纵杆方向和按钮
  - P<sub>sa</sub>是状态转移概率
    - 对于每个状态和动作是下一个状态在S中的概率分布
  - Y是对未来奖励的折扣因子
  - R是奖励函数
    - 有时奖励只和状态有关

## 基于动态规划的强化学习

### MDP目标和策略

- 目标：==选择能够最大化累积奖励期望的动作==

- 给定一个特定的策略
  - 在状态s下采取动作
- 给定策略定义==价值函数==
  - 给定初始状态和根据策略采取动作时的累积奖励期望

### 最优价值函数

- 对状态s来讲，最优价值函数是==所有策略可以获得的最大可能折扣奖励和==

### 价值迭代和策略迭代

- 可以对==最优价值函数==和==最优策略==执行迭代更新
  - 价值迭代
  - 策略迭代

### 同步 VS 异步价值迭代

- 同步的价值迭代会储存两份价值函数的拷贝
- 异步价值迭代只储存一份价值函数

### 价值迭代 VS 策略迭代

1. 价值迭代是贪心更新法
2. 策略迭代中，用Bellman等式更新价值函数代价很大
3. 对于空间较小的MDP，策略迭代通常更新很快收敛
4. 对于空间较大的，价值迭代更实用（效率更高）
5. 如果没有状态转移循环，最好使用价值迭代

